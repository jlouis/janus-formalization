\chapter{\janusz{}}

The \janusz{} programming language is a subset of the full JANUS
language from \cite{glueck2007}. It not a Turing Complete language and
contains only simple linear constructs. However, it contains enough to
provide a vehicle for explaining the basics of a formalization. We aim
to provide a big step operational semantics for the language, suitable
for encoding into the logical framework \coq{}.

Describing a language requires us to define the basic object that is
manipulated. There are two such objects in \janusz{}: integers and
stores.

The integers are the mathematical integers, ie drawn from $\ZZ$. In
full JANUS the primary object are 32-bit integers but in our
simplified version, we just use usual integers.
\begin{defn}
  \label{defn-lift}
  For any set, $S$ we define its domain-theoretic \emph{lift} to be
  $S_{\perp} = S \cup \{\perp\} $ for a special value $\perp$ called
  ``bottom''. Values $s \in S$ are notated as $\lift{s}$ which is
  called the ``lift''.
\end{defn}
Information-theoretically, the bottom values represents ``no
information'' whereas the lift of a value represents that value. This
is akin to the well-known ML datatype ``option''.

The stores, notated as $\sigma, \sigma', \dotsc$ are functions from
natural numbers to lifted integers: $\sigma \colon \NN \to
\ZZ_{\perp}$. The domain of natural numbers are called
\emph{locations}. We usually give them names $x, y, z, \dotsc$ and
assume each variable is associated with a specific number for the
duration of the program. The co-domain defines the contents of the
current location in the store. These are called \emph{values}. If no
value is associated with the location $x$ then $x \mapsto \perp$ and
otherwise $x \mapsto \lift{i}$ for some $i \in \ZZ$. This process is
used as lookup.

Stores are altered under the course of running a program. In our
formalization this amounts to changing the function
$\sigma$. The notation $\sigma[x \mapsto k]$ means ``update the contents
of location $x$ with the value $k$''. The mathematical formal meaning
is:
\begin{equation*}
  \sigma[x \mapsto k](z) = \begin{cases}
    \lift{k} & \quad x = z\\
    \sigma(z)  & \quad \text{otherwise}
  \end{cases}
\end{equation*}

The empty store $\epsilon$ maps everything to $\perp$, ie. for all
locations $l \in \ZZ$ we have $\epsilon(l) = \perp$. We will use the
empty store as the inital store.

It will be beneficial to provide \emph{hiding} of values in the store. A
hiding of a variable $x$ is a new store in which $x$ does not
exist. Because we have the value $\perp$ in our store-values, there is
a straightforward definition. We use the notation $(\sigma \setminus x)$
for the hiding of the variable $x$ in the store $\sigma$. The definition
is:
\begin{equation*}
  (\sigma \setminus x)(z) = \begin{cases}
    \perp & \quad z = x\\
    \sigma(z) & \quad \text{otherwise}
  \end{cases}
\end{equation*}

\subsection{Coq encoding}

One encodes the semantics of object languages in the language of
Gallina, the specification language of \coq{}. This language is
inherently functional and is furthermore \emph{total} for its
inductive part\footnote{We are not using the CoInductive features of
  \coq{}}. Functional programmers should feel at ease when working
with the language as it works like a normal programming language. But
since the language is based on the Calculus of (Co-)Inductive
constructions, the type system is far more advanced than one a
functional programmer is accustomed to. For \coq{} version 8.2 you
will find most of the advanved type concepts either directly in the
type system, or derivable. \coq{} supports dependent types, the
polymorphic lambda calculus, higher order types, Haskell-style type
classes, ML style module system with functors, GADTs etc.\fixme{cite,
  cite, cite}

In \coq{} there are already integers built-in, ready for use in our
formaliztion. This means we can focus entirely on providing an
implementation of stores.

In the stores given above in, we operate with a domain of $\NN$ and a
codomain of $\ZZ$ used as the locations and values respectively. For
encoding this in \coq{}, we define a \texttt{Module Type}. For ML
people this is a type signature. The implementation is:
\begin{verbatim}
Module Type STORE.

  Parameter location : Set. (* Domain of the store mapping *)
  Parameter value : Set.    (* Codomain of the store mappring *)

  (* locations have equality *)
  Parameter eq : location -> location -> Prop.
  Parameter location_eq_dec :
      forall (n m : location), {n = m} + {n <> m}.

End STORE.
\end{verbatim}
In this definition we define two parameters, locations and
values. Both belong to the ``Set'' world, which is the world of
objects in \coq{}. The \texttt{eq} parameter specifies an equality
predicate on locations. It returns its result as a ``Prop'' which is
the world of propositions in the universe. The distinction between
``Set'' and ``Prop'' is made such that we keep the language and its
properties apart in the formalization.

Finally, we require the existence an equality decider for
locations. It states that either $n = m$ or $n \neq m$ for locations
$n, m$.

Memories are defined as a \emph{module functor} expecting a
\texttt{STORE} fulfilling module as input and producing a
\texttt{Memory} as output. By using a functor we hoist the specifics
of the domain and codomain of the memory, so we can change the
definition later on.

The memory is a function from locations to values $S_{\perp}$, encoded
as an option type as known from ML. This encodes a lift as given in
Definition \ref{defn-lift}. The value of $\perp$ is encoded as
\texttt{None} and the lift $\lift{s}$ as \texttt{Some s}. The
definition of the empty store is then straightforward, where locations
are named ``var'' for convenience:
\begin{verbatim}
  Definition empty (_ : var) : option value := None.
\end{verbatim}
Lookup on a memory is function application of the location to the
memory function. Writing a new value to a given location is happening
according the update function given above:
\begin{verbatim}
  Definition write (m : memory) x v x' :=
    if location_eq_dec x x'
      then Some v
      else m x'.
\end{verbatim}
Hiding is also carried out according to the matematical definition.

\subsection{Properties of stores}

The stores we have defined has a set of properties associated with
them. These properties are important when we want to prove theorems
about \janusz{} as they form the \emph{Knowledge basis} for the
stores. The knowledge basis is the set of properties we build on when
we formalize properties of \janusz{}, but are not directly
related. When humans carry out proofs there is an implicit temptation
to assume the existence of most of this work by intuition. For a
machine however, we will have to provide it with the theorems as well
as the proofs. The proofs presented here is a selection from the
development.

\begin{lem}
  \label{lem:write-eq}
  For a store $\sigma$ we have $\sigma[x \mapsto v](v) = \lift{v}$
\end{lem}
\begin{proof}
  In \coq{} (case analysis on $x$ after unfolding the definition of updates).
\end{proof}
Theorem \ref{lem:write-eq} has a similar proposition:
\begin{lem}
  Assume a store $\sigma$ and let $x, y$ be locations with $x \neq
  y$. Then we have, for any value $v$:
  \begin{equation*}
    \sigma[x \mapsto v](y) = \sigma(y)
  \end{equation*}
\end{lem}
\begin{proof}
  By \coq{}.
\end{proof}

Naturally, the above statement can be extended to equalities, which
will be used later on:
\begin{lem}
\label{lem:write-eq2}
  We have:
  \begin{multline*}
    \forall \sigma, \sigma' \in \Sigma, x \in Loc, v_1, v_2 \in Value
    \colon \\
    \sigma[x \mapsto v_1](x) = \sigma[x \mapsto v_2](x) \implies v_1 = v_2
  \end{multline*}
\end{lem}
\begin{proof}
  By \coq{}
\end{proof}

The following property states that ``if you have enough pieces placed
correctly you have the whole puzzle''; that is, if you enough about a
store, you can decide its equality:
\begin{lem}
\label{lem:hide-eq}
  Assume two arbitrary stores $\sigma, \sigma' \in \Sigma$. Assume a
  variable $x$ and a value $v$ from any value domain. Now suppose
  \begin{itemize}
  \item $\sigma(x) = v$
  \item $\sigma'(x) = v$
  \item $(\sigma \setminus x) = (\sigma' \setminus x)$
  \end{itemize}
  Then $\sigma = sigma'$
\end{lem}
\begin{proof}
  By \coq{}.
\end{proof}

Another lemma on stores establishes a property when we are hiding
information:
\begin{lem}
\label{lem:hide_ne}
  Assume a store $\sigma \in \Sigma$ and two variables $x$ and
  $x'$. If $x \neq x'$ then we have $(\sigma \setminus x)(x') = \sigma(x')$.
\end{lem}
\begin{proof}
  In \coq{}, by the definition of hiding and case analysis.
\end{proof}

The following lemma will come in handy later. It is based on hiding
stores:
\begin{lem}
\label{lem:write_hide}
  Assume two stores $\sigma, \sigma' \in \Sigma$, a variable $x$ and
  two values $v_1, v_2 \in V$, for any value domain $V$. Then
  $\sigma[x \mapsto v_1] = \sigma'[x \mapsto v_2]$ implies $(\sigma
  \setminus x) = (\sigma' \setminus x)$
\end{lem}
\begin{proof}
  In \coq{}. The proof relies on backwards reasoning. It uses
  extensionality (see \ref{coqext:extensionality}) and then it uses
  case analysis. In the cases, it applies lemma \eqref{lem:hide_ne}.
\end{proof}

Another very important concept is that a write to the hidden variable
does not matter. The following lemma makes this specific
\begin{lem}
\label{lem:hide-write}
  Let there be given an arbitrary store $\sigma$, variable $x$ and value
  $v$. Then the following holds:
  \begin{equation*}
    (\sigma[x \mapsto v] \setminus x) = (\sigma \setminus x)
  \end{equation*}
\end{lem}
\begin{proof}
  The proof utilizes extensionality to discern each possible element
  in the maps. Then it discriminates on wether we are working with $x$
  or not. For each case, a simple computation discharges the subproblem.
\end{proof}

\subsection{History}

In the course of construction the right store, we tried several
experiments which all proved to be less useful than the development
given here. At the beginning, we envisioned the store to be a map from
$Loc \to S$. The empty store were then defined as mapping everything
to the value $0$. The problem with this solution is that we cannot
differentiate between the value $0$ and no value at all. This
eliminates our knowledge of a location being invalid.

We ultimately changed to the store representation given, when it
became apparent we needed the ability to \emph{hide} certain store
locations. An ``option''-type was then the only right implementation.

\section{Expressions in \janusz{}}

The \janusz{} language has a very simplified expression language
compared to full JANUS. In this language there are 5 expression
constructs: integer constants, store referencing, addition,
subtraction and multiplication.

The syntax of expressions $e$ is the following in BNF notation:
\newcommand{\bor}{\; | \;}
\begin{equation*}
  e ::= n \bor \mathtt{x} \bor e + e \bor e - e \bor e * e
\end{equation*}
The judgement forms are $\sigma |- e => z$ stating that under the
assumption of a store $\sigma$ the expression $e$ evaluates to the
integer $z$. The inference rules for this system is straightforward:
\begin{gather*}
  \inference[Const]{}{\sigma |- n => n} \quad \inference[Var]{\sigma(\mathtt{x}) =
    \lift{k}}{\sigma |- \mathtt{x} => k} \\
  \inference[Add]{\sigma |- e_1 => n_1 \quad \sigma |- e_2 => n_2 \quad
    n_1 + n_2 = n}{\sigma |- e_1 + e_2 => n} \\
  \inference[Sub]{\sigma |- e_1 => n_1 \quad \sigma |- e_2 => n_2 \quad
    n_1 - n_2 = n}{\sigma |- e_1 - e_2 => n} \\
  \inference[Mul]{\sigma |- e_1 => n_1 \quad \sigma |- e_2 => n_2 \quad
    n_1 * n_2 = n}{\sigma |- e_1 * e_2 => n}
\end{gather*}

There is another, denotational, semantics for expressions
however. This semantics are equal to the operational semantics given
above. One aspect of Coq is that it is more natural to express
denotational semantics than operational semantics.

\label{exp:denot-semantics}
For a denotational semantics, we define a computation
$\mathcal{E}|[-|] \colon E \to \Sigma \to \ZZ_{\perp}$ from an
expression and a Store $\Sigma$ to a lifted value. The definition is a
case analysis on the structure of the expression given:
\begin{align*}
  \mathcal{E}|[n|](\sigma) & = \lift{n}\\
  \mathcal{E}|[x|](\sigma) & = \sigma (x)\\
  \mathcal{E}|[e_1 + e_2|](\sigma) & = \mathcal{E}|[e_1|](\sigma) \;
  +_{\ZZ} \;
  \mathcal{E}|[e_2|](\sigma)\\
  \mathcal{E}|[e_1 - e_2|](\sigma) & = \mathcal{E}|[e_1|](\sigma) \;
  -_{\ZZ} \;
  \mathcal{E}|[e_2|](\sigma)\\
  \mathcal{E}|[e_1 * e_2|](\sigma) & = \mathcal{E}|[e_1|](\sigma) \;
  *_{\ZZ} \;
  \mathcal{E}|[e_2|](\sigma)
\end{align*}
The operation $+$ is the operation from our expression language
whereas the operation $+_{\ZZ}$ is the mathematical integer addition
here made explicit by its annotation. Likewise is the case for $-$ and
$*$.

The advantage of the latter, denotational, definition is that is
allows for simpler proofs in Coq. Basically a standard Case analysis
will do over the structure. Operational semantics uses a Prolog-style
where a relation between the premises and conclusion is defined. The
advantage of this prolog style is of course its generality. The
denotational style above is a function definition based on case
analysis which is a special case of a relation.

If $R \subseteq X \times Y$ is a relation on $X$ and $Y$, then the
relation would be a function in the folowing case: if $(a, b) \in R$
and also $(a, c) \in R$ then we must have $b = c$. In other words, a
function is deterministic in its output based on its output.

\subsection{Encoding expressions in \coq{}}

The syntax of \janusz{}-expressions is encodable as a normal datatype
known from e.g. ML og Haskell. These are called inductive types in
\coq{} and happens to be much more powerful than their ML
counterparts. Here, we will only use the ML-subset though.
\begin{verbatim}
    Inductive Exp : Set :=
    | Exp_Const : Z -> Exp
    | Exp_Var : Var -> Exp
    | Exp_Add : Exp -> Exp -> Exp
    | Exp_Sub : Exp -> Exp -> Exp
    | Exp_Mul : Exp -> Exp -> Exp.
\end{verbatim}

The denotational expression evaluation of \janusz{} can be encoded as a
\texttt{Fixpoint}. We have to show that the fixpoint is total, but
this is rather simple as we always decrease the size of the expression
when we evaluate it. Here we give the start of the definition and
explain it:
\begin{verbatim}
    Fixpoint denote_Exp (m : ZMem.memory) (e : Exp) {struct e}
        : option Z :=
      match e with
        | Exp_Const z => Some z
        | Exp_Var x => m x
        | Exp_Add e1 e2 =>
            match (denote_Exp m e1, denote_Exp m e2) with
              | (Some n1, Some n2) => Some (n1 + n2)
              | _ => None
            end
        ...
\end{verbatim}
This describes expression annotations as a fixpoint (recursive
function) taking a memory, and an expression as inputs. It produces a
value of type $\ZZ_{\perp}$ as output. The
\texttt{struct}-designation defines the input that gets gradually
smaller on recursion, which \coq{} will utilize for obtaining a
termination proof.

The function itself is a simple case match on the different
constructors of the inductive type. Certain constructors have been
omitted in the above fragment.

\subsection{Properties of \janusz{} expressions}

We will only need the forward determinism for expressions and hence
there is no need for the generality of the relation. We therefore
employ a denotational definition as it greatly simplifies the needed
proof of forward determinism:

\begin{thm}
  \janusz{} expressions are forward deterministic: suppose we have a
  store $\sigma$ and an expression $e$. Now let
  $\mathcal{E}|[e|](\sigma) = v_1$ for a value $v_1 \in \ZZ_{\perp}$
  and also let $\mathcal{E}|[e|](\sigma) = v_2$ for a value $v_2 \in
  \ZZ_{\perp}$. Then we have $v_1 = v_2$.
\end{thm}
\begin{proof}
By the use of \coq{} and the \texttt{grind}-tactic from
\cite{chlipala+08:cpdt}:
\begin{verbatim}
    Theorem exp_determ : forall m e v1 v2,
      denote_Exp m e = v1 -> denote_Exp m e = v2 -> v1 = v2.
    Proof.
      grind.
    Qed.
\end{verbatim}
\end{proof}
The proof of determinism shows the benefit of using a denotational
semantics for expressions. Since we are essentially defining our
function to be total in the language of the CoIC used by Coq, we gain
forward determinism for free by borrowing it from the underlying
language.

\section{Statements in \janusz{}}

The statement syntax of \janusz{} defines a small, purely linear
subset of the full JANUS language. There are 4 constructs given via
the following notation in BNF:
\reservestyle{\command}{\mathbf}
\command{if[\;if\;],then[\;then\;],else[\;else\;],fi[\;fi\;],skip[\;skip\;],
 +=[\;+\!\!=\;],-=[\;-\!\!\!=\;],call[\;call\;],uncall[\;uncall\;]}
\begin{gather*}
  s ::= \quad \<skip> \bor x \<+=> e \bor x \<-=> \bor s; s
  \bor \<if> e \<then> s \<else> s \<fi> e
\end{gather*}
\newcommand{\angel}[1]{\langle #1 \rangle}

We will interleave the description of each of these syntactical
elements with the inference rules for them. The judgement form for
execution of statements is $\angel{\sigma, s} -> \sigma'$ which
designates that under the assumption of a store $\sigma$, execution of
statement $s$ will yield the altered store $\sigma'$.

The very first operation is a simple one. The \textbf{skip} operation
is a no-operation command that simply skips. It has the following
rule:
\begin{equation*}
  \inference[Skip]{}{\angel{\sigma, s} -> \sigma}
\end{equation*}

The second operation is the increment operation of an element in the
store. This operation, written as $x +\!\!= e$ will evaluate the
expression $e$ to a number $k$ and then add this amount to the
location in the store to which $x$ points:
\begin{equation*}
  \inference[Inc0]{\sigma |- e => k \quad \sigma(x) = \lift{k'} \quad k +
    k' = n}{\angel{\sigma, x +\!\!= e} -> \sigma[x \mapsto n]}
\end{equation*}
In JANUS it is a requirement that the variable $x$ must not occur in
the expression $e$. The same requiremnt is present in \janusz{} and
has to do with the invertibility of such statements. There is,
however, an alternative semantics not present in the current
literature which directly encodes the requirement in the inference
rule.

Recall we defined an ability to ``hide'' certain variables in our
store. We can utilize this by hiding $x$ in the expression evaluation:
\begin{equation*}
  \inference[Inc]{(\sigma \setminus x) |- e => k \quad \sigma(x) =
    \lift{k'} \quad k + k' = n}{\angel{\sigma, x +\!\!= e} -> \sigma[x \mapsto n]}
\end{equation*}
Now, because expression evaluation of the ``Var'' case requires a
lifted value it is now impossible to construct an inference tree where
the expression $e$ refers to the value $x$. We have effectively
encoded the informal requirement into a formal one. This method,
correctness by construction, simplifies proof formalization: had we
chosen a predicate judgement for an non-occurring variable $x$, then
we would have to provide a proof with this added structure.

For decrementation, the definition is similar:
\begin{equation*}
  \inference[Dec]{(\sigma \setminus x) |- e => k \quad \sigma(x) =
    \lift{k'} \quad k - k' = n}{\angel{\sigma, x -\!\!= e} -> \sigma[x \mapsto n]}
\end{equation*}

The next rule is for sequencing operations. In the statement $s_1;
s_2$ one first executes $s_1$ and then feeds the resulting store into
the execution of $s_2$:
\begin{equation*}
  \inference[Seq]{\angel{\sigma, s_1} -> \sigma'' \quad
    \angel{\sigma'', s_2} -> \sigma'}
  {\angel{\sigma, s_1; s_2} -> \sigma'}
\end{equation*}

Finally, there is the rule for the branching instruction. In \janusz{}
the value $0$ is ``false'' and any value different from $0$ is the
``true'' value. This yields two rules, one for each case. The first
rule is for the ``false'' case:
\begin{equation*}
  \inference[If-false]{\sigma |- e_1 => 0 \quad \angel{\sigma, s_2} -> \sigma'
    \quad \sigma' |- e_2 => 0}{\angel{\sigma, \<if> e_1 \<then> s_1 \<else> s_2 \<fi> e_2} -> \sigma'}
\end{equation*}
This rule states that if the $e_1$ \emph{test} evaluates to a false
value, then the ``else''-branch is executed. Finally the
\emph{assertion} $e_2$ must be false as well.

The true case is similar, the difference being extra (mathematical)
requirements on what the expressions evaluates to:
\begin{equation*}
  \inference[If-true]{\sigma |- e_1 => k \quad k \neq 0 \quad
    \angel{\sigma, s_1} -> \sigma'
    \quad \sigma' |- e_2 => k' \quad k' \neq 0}{\angel{\sigma, \<if> e_1 \<then> s_1 \<else> s_2 \<fi> e_2} -> \sigma'}
\end{equation*}

The need for assertions has to do with reversibility, as we shall see
later on.
\subsection{Encoding statements in Coq}

Encoding of the statement syntax is rather straightforward. We use an
inductive type to capture the valid syntax rules. We omit it here for
brevity as it is pretty simple and uninteresting.

Encoding the inference rule system for \janusz{} is more
interesting. To do this, we also use an inductive type, but make it a
member of the ``Prop'' world of propositions. Furthermore, we use a
\emph{dependent} type to build a family of types for inference
rules.\fixme{cite, cite, cite! (ATTAPL}

We make \texttt{Stm\_eval} inductive with the type signature $mem \to
Stm \to mem \to \mathbf{Prop}$ which makes it dependent on the input
memory, the statement executing and the resulting memory. This is done
in \coq{} by:
\begin{verbatim}
Inductive Stm_eval : mem -> Stm -> mem -> Prop :=
...
\end{verbatim}

Next, we encode each inference rule as a constructor of this inductive
type. For instance, we encode the skip rule as:
\begin{verbatim}
| se_skip: forall m, Stm_eval m S_Skip m
\end{verbatim}
This states that for all memories $m$, we have that skip yields $m$ as
the resulting memory. Think about it as a prolog-style relation on
statement evaluation.

Increment uses some premises:
\begin{verbatim}
| se_assvar_incr: forall (m m': mem) (v: Var)
   (z z' r: Z) (e: Exp),
  denote_Exp (ZMem.hide m v) e = Some z ->
  m v = Some z' ->
  r = (z + z') ->
  m' = ZMem.write m v r ->
    Stm_eval m (S_Incr v e) m'
\end{verbatim}
Note how each premise is encoded as an assumption of the
constructor. If all premises are fulfilled, then we have a relation on
$m$, $\text{S\_Incr}\; v\; e$ and $m'$. The other rules are similar.

\subsection{Proving determinism of \janusz{}}

For \janusz{}, we will prove a couple of theorems. The 2 main theorems
to be proven are those of forward and backward determinism. The
language must be deterministic in the forward direction, so there is
only one possible outcome of any computation. Also, it must be
deterministic in the backwards direction: if we have a computation
resulting in a resulting store, the original store must be equivalent.

It is clear this establishes a necessary condition for
reversibility. Had the language not been backwards deterministic, it
would be outright impossible to reconstruct the input store from the
output store.
\begin{lem}
\label{j0-fwd-det-prime}
  Let $\sigma, \sigma' \in \Sigma$ be stores. Let $s$ be any \janusz{}
  statement. Now, if $\angel{\sigma, s} -> \sigma'$ implies for all $\sigma''
  \in \Sigma$ we have $\angel{\sigma, s} -> \sigma''$, then $\sigma' =
  \sigma''$. Formally:
  \begin{equation*}
    \forall \sigma, \sigma' \in \Sigma, s \in Stm \colon
    \angel{\sigma, s} -> \sigma' \implies (\forall \sigma'' \in \Sigma
    \colon \angel{\sigma, s} -> \sigma'' \implies \sigma' = \sigma'')
  \end{equation*}
\end{lem}
\begin{proof}
  In \coq{}.
\end{proof}

The proof proceeds by induction over the 1st dependent hypothesis, ie
$\angel{\sigma, s} -> \sigma'$. The increment and decrement cases are
similar. They proceed by noting (for increment) that $(\sigma
\setminus x) |- e => k$ occurs in the premise of $\angel{\sigma, s} ->
\sigma''$ as well, so the $k$ must be identical. The same is the case
for $\sigma(x) = \lift{k'}$ and hence $k + k'$ is written in both
cases.

The semi-case just aspires to the induction hypothesis. Due to the way
the lemma is worded we have $\forall \sigma'' \colon \angel{\sigma, s}
-> \sigma''$ as the induction hypothesis and then this is trivially
proven.

The if-case is done by inversion of $\angel{\sigma, \<if> e1 \<then> s1
\<else> s2 \<fi> e2} -> \sigma''$. One of these are solvable by the
induction hypothesis. The other is solvable because it is an
impossible case (false-inversion occuring while proving the if-true
subgoal -- or true-inversion when proving the if-false subgoal).

\begin{thm}
\label{thm:j0-fwd-det}
  \janusz{} is forward deterministic, ie:
  \begin{equation*}
    \forall \sigma, \sigma', \sigma'' \in \Sigma, s \in Stm \colon \angel{\sigma, s} -> \sigma' \implies \angel{\sigma, s} -> \sigma'' \implies \sigma' = \sigma''
  \end{equation*}
\end{thm}
\begin{proof}
  The $\forall \sigma''...$ is hoistable in lemma
  \eqref{j0-fwd-det-prime} due to logic rules. This is made formal in
  \coq{} by introducing everything but $\sigma' = \sigma''$ as as
  hypotheses and then generalizing over the form needed to apply lemma
  \eqref{j0-fwd-det-prime}.
\end{proof}

In the forward determinism-proof nothing special has been applied to
carry out the proof. Either one can calculate (in the integer ring
$\ZZ$) and conclude the result by identification or one can apply the
induction hypothesis to obtain the result after inversion. As we shall
see, the backwards determistic proof is not as simple, though it
holds:

\begin{lem}
  Let $\sigma, \sigma' \in \Sigma$ be stores. Let $s \in Stm$ be any
  statement. Assume $\angel{\sigma', s} -> \sigma$ (note the position of
  $\sigma'$, compare with lemma \eqref{j0-fwd-det-prime}). Then
  $\forall \sigma'' \colon \angel{\sigma'', s} -> \sigma$ implies $\sigma' =
  \sigma''$. Formally:
  \begin{equation*}
    \forall \sigma, \sigma' \colon \angel{\sigma', s} -> \sigma \implies
    (\forall \sigma'' \colon \angel{\sigma'', s} -> \sigma \implies \sigma'
    = \sigma'')
  \end{equation*}
\end{lem}
\begin{proof}
  Via \coq{}
\end{proof}

Completing this proof requires considerably more ingenuity than the
forward proof. Again, we run induction on the 1st dependent
hypothesis. For the increment case, we first run inversion, and
identify equal terms via the \texttt{subst} tactic. We must have
$(\sigma' \setminus x) = (\sigma'' \setminus x)$ because we have
$\sigma'[x \mapsto k + k'] = \sigma''[x \mapsto k_0 + k_0']$ (lemma
\eqref{lem:write_hide}). This means, that $k = k_0$. Next, we have $k
+ k' = k_0 + k_0'$. This is because of lemma \eqref{lem:write-eq2} and
analysis on $\sigma'[x \mapsto k + k'] = \sigma''[x \mapsto k_0 +
k_0']$. Computation in the ring $\ZZ$ now gives that $k' =
k_0'$. After substitution, we then use lemma \eqref{lem:hide-eq}
proving the case. The decrement case is equivalent.

The cases using the induction hypothesis (semi/if) are trivial,
utilizing the \texttt{congruence} tactic to solve the impossible cases
in the evaluation of the ``if'' cases.

\begin{thm}
  \janusz{} is backward deterministic, ie:
  \begin{equation*}
    \forall \sigma, \sigma', \sigma'' \in \Sigma, s \in Stm \colon
    \angel{\sigma', s} -> \sigma \implies \angel{\sigma'', s} -> \sigma \implies \sigma' = \sigma''
  \end{equation*}
\end{thm}
\begin{proof}
  Like the proof of theorem \eqref{thm:j0-fwd-det}, we can hoist the
  forall-quantifier and formalize it in \coq{}.
\end{proof}

\section{Inversion of \janusz{}}

In order to invert \janusz{}, we define a rewriting system
$\mathcal{I}(\cdot) \colon Stm \to Stm$ from statements to
statements. The rewriting system is straightforwardly defined: note
that each atomic operation has a in inverse in the language and that
each compound statement can be inductively inverted:
\begin{align*}
  \mathcal{I}(\<skip>)& = \<skip>\\
  \mathcal{I}(x \<+=> e)& = x \<-=> e\\
  \mathcal{I}(x \<-=> e)& = x \<+=> e\\
  \mathcal{I}(s1; s2)& = \mathcal{I}(s2); \mathcal{I}(s1)\\
  \mathcal{I}(\<if> e_1 \<then> s_1 \<else> s_2 \<fi> e_2)& = \<if> e_2 \<then> \mathcal{I}(s_1) \<else> \mathcal{I}(s_2) \<fi> e_1
\end{align*}

The first sanity-check on the rewriting system is that of
self-isomorphism; applying the rewriting system twice yields the
original:
\begin{thm}
  For any statement $s$, we readily have:
  \begin{equation*}
    \mathcal{I}(\mathcal{I}(s)) = s
  \end{equation*}
\end{thm}
\begin{proof}
  By induction over $s$ in \coq{}.
\end{proof}

In the paper \cite{glueck2007} an equivalence relation, $e_1 \sim e_2$,
is created between expressions $e_1$ and $e_2$ by
\begin{equation*}
  e_1 \sim e_2 \equiv \forall v \in Value, \sigma \in \Sigma \colon
  \sigma |- e_1 => v <=> \sigma |- e2 => v
\end{equation*}
Similarly, a relation $s_1 \sim s_2$ is created between statements
$s_1$ and $s_2$ by
\begin{equation*}
  s_1 \sim s_2 \equiv \forall \sigma, \sigma' \in \Sigma \colon
  \angel{\sigma, s_1} -> \sigma' <=> \angel{\sigma, s_2} -> \sigma'
\end{equation*}

We have defined these in \coq{} for \janusz{} and shown them to be
equivalence relations indeed. It can then be proven that the
sequencing in \janusz{} is associative: $s_1; (s_2; s_3) \sim (s_1;
s_2); s_3$. This proof was carried out in \coq{}. In the paper
associative property among others is used to prove the following theorem:
\begin{thm}
  Let $s$ be an arbitrary \janusz{} statement and let $\sigma$ and
  $\sigma'$ be arbitrary stores. The following is then true:
  \begin{equation*}
    \angel{\sigma, s} -> \sigma' \iff \angel{\sigma', \mathcal{I}(s)} -> \sigma
  \end{equation*}
\end{thm}

At least for \janusz{} however, you don't need the equivalence
relation at all. Perhaps it is needed when running induction on the
statement $s$ as is done in the paper, but another path is to run a
generalized induction hypothesis on $\forall \sigma, \sigma' \in
\Sigma, s \in Stm, \angel{\sigma, s} -> \sigma' \implies
\angel{\sigma', \mathcal{I}(s)} -> \sigma$ (and the converse in the other
direction). This allows you to apply the induction hypothesis in the
sequence case \texttt{semi} in the obvious way and stitch the result
together. The hardest part of proving this statement in \coq{} is to
get the math right in the inversion of $\<+=>$ and $\<-=>$. You need
several properties of stores, including lemmas \eqref{lem:hide-write},
\eqref{lem:hide-eq} and \eqref{lem:write-eq} to mangle the store into
place.
\begin{proof}
  In \coq{}.
\end{proof}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
