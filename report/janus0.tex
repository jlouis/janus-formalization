\chapter{\janusz{}}

The \janusz{} programming language is a subset of the full JANUS
language from \cite{glueck2007}. It not a Turing Complete language and
contains only simple linear constructs. However, it contains enough to
provide a vehicle for explaining the basics of a formalization. We aim
to provide a big step operational semantics for the language, suitable
for encoding into the logical framework \coq{}.

Describing a language requires us to define the basic object that is
manipulated. There are two such objects in \janusz{}: integers and
stores.

The integers are the mathematical integers, ie drawn from $\ZZ$. In
full JANUS the primary object are 32-bit integers but in our
simplified version, we just use usual integers.
\begin{defn}
  \label{defn-lift}
  For any set, $S$ we define its domain-theoretic \emph{lift} to be
  $S_{\perp} = S \cup \{\perp\} $ for a special value $\perp$ called
  ``bottom''. Values $s \in S$ are notated as $\lift{s}$ which is
  called the ``lift''.
\end{defn}
Information-theoretically, the bottom values represents ``no
information'' whereas the lift of a value represents that value. This
is akin to the well-known ML datatype ``option''.

The stores, notated as $\sigma, \sigma', \dotsc$ are functions from
natural numbers to lifted integers: $\sigma \colon \NN \to
\ZZ_{\perp}$. The domain of natural numbers are called
\emph{locations}. We usually give them names $x, y, z, \dotsc$ and
assume each variable is associated with a specific number for the
duration of the program. The co-domain defines the contents of the
current location in the store. These are called \emph{values}. If no
value is associated with the location $x$ then $x \mapsto \perp$ and
otherwise $x \mapsto \lift{i}$ for some $i \in \ZZ$. This process is
used as lookup.

Stores are altered under the course of running a program. In our
formalization this amounts to changing the function
$\sigma$. The notation $\sigma[x \mapsto k]$ means ``update the contents
of location $x$ with the value $k$''. The mathematical formal meaning
is:
\begin{equation*}
  \sigma[x \mapsto k](z) = \begin{cases}
    \lift{k} & \quad x = z\\
    \sigma(z)  & \quad \text{otherwise}
  \end{cases}
\end{equation*}

The empty store $\epsilon$ maps everything to $\perp$, ie. for all
locations $l \in \ZZ$ we have $\epsilon(l) = \perp$. We will use the
empty store as the inital store.

It will be beneficial to provide \emph{hiding} of values in the store. A
hiding of a variable $x$ is a new store in which $x$ does not
exist. Because we have the value $\perp$ in our store-values, there is
a straightforward definition. We use the notation $(\sigma \setminus x)$
for the hiding of the variable $x$ in the store $\sigma$. The definition
is:
\begin{equation*}
  (\sigma \setminus x)(z) = \begin{cases}
    \perp & \quad z = x\\
    \sigma(z) & \quad \text{otherwise}
  \end{cases}
\end{equation*}

\subsection{Coq encoding}

In \coq{} there are already integers built-in, ready for use in our
formaliztion. This means we can focus entirely on providing an
implementation of stores.

In the stores given above in, we operate with a domain of $\NN$ and a
codomain of $\ZZ$ used as the locations and values respectively. For
encoding this in \coq{}, we define a \texttt{Module Type}. For ML
people this is a type signature. The implementation is:
\begin{verbatim}
Module Type STORE.

  Parameter location : Set. (* Domain of the store mapping *)
  Parameter value : Set.    (* Codomain of the store mappring *)

  (* locations have equality *)
  Parameter eq : location -> location -> Prop.
  Parameter location_eq_dec : forall (n m : location), {n = m} + {n <> m}.

End STORE.
\end{verbatim}
In this definition we define two parameters, locations and
values. Both belong to the ``Set'' world, which is the world of
objects in \coq{}. The \texttt{eq} parameter specifies an equality
predicate on locations. It returns its result as a ``Prop'' which is
the world of propositions in the universe. The distinction between
``Set'' and ``Prop'' is made such that we keep the language and its
properties apart in the formalization.

Finally, we require the existence an equality decider for
locations. It states that either $n = m$ or $n \neq m$ for locations
$n, m$.

Memories are defined as a \emph{module functor} expecting a
\texttt{STORE} fulfilling module as input and producing a
\texttt{Memory} as output. By using a functor we hoist the specifics
of the domain and codomain of the memory, so we can change the
definition later on.

The memory is a function from locations to values $S_{\perp}$, encoded
as an option type as known from ML. This encodes a lift as given in
Definition \ref{defn-lift}. The value of $\perp$ is encoded as
\texttt{None} and the lift $\lift{s}$ as \texttt{Some s}. The
definition of the empty store is then straightforward, where locations
are named ``var'' for convenience:
\begin{verbatim}
  Definition empty (_ : var) : option value := None.
\end{verbatim}
Lookup on a memory is function application of the location to the
memory function. Writing a new value to a given location is happening
according the update function given above:
\begin{verbatim}
  Definition write (m : memory) x v x' :=
    if location_eq_dec x x'
      then Some v
      else m x'.
\end{verbatim}
Hiding is also carried out according to the matematical definition.

\subsection{Properties of stores}

The stores we have defined has a set of properties associated with
them. These properties are important when we want to prove theorems
about \janusz{} as they form the \emph{Knowledge basis} for the
stores. The knowledge basis is the set of properties we build on when
we formalize properties of \janusz{}, but are not directly
related. When humans carry out proofs there is an implicit temptation
to assume the existence of most of this work by intuition. For a
machine however, we will have to provide it with the theorems as well
as the proofs. The proofs presented here is a selection from the
development.

\begin{lem}
  \label{lem:write-eq}
  For a store $\sigma$ we have $\sigma[x \mapsto v](v) = \lift{v}$
\end{lem}
\begin{proof}
  In \coq{} (case analysis on $x$ after unfolding the definition of updates).
\end{proof}
Theorem \ref{lem:write-eq} has a similar proposition:
\begin{lem}
  Assume a store $\sigma$ and let $x, y$ be locations with $x \neq
  y$. Then we have, for any value $v$:
  \begin{equation*}
    \sigma[x \mapsto v](y) = \sigma(y)
  \end{equation*}
\end{lem}
\begin{proof}
  By \coq{}.
\end{proof}

Naturally, the above statement can be extended to equalities, which
will be used later on:
\begin{lem}
\label{lem:write-eq2}
  We have:
  \begin{equation*}
    \forall \sigma, \sigma' \in \Sigma, x \in Loc, v_1, v_2 \in Value
    \colon \sigma[x \mapsto v_1](x) = \sigma[x \mapsto v_2](x)
    \implies v_1 = v_2
  \end{equation*}
\end{lem}
\begin{proof}
  By \coq{}
\end{proof}

The following property states that ``if you have enough pieces placed
correctly you have the whole puzzle''; that is, if you enough about a
store, you can decide its equality:
\begin{lem}
\label{lem:hide-eq}
  Assume two arbitrary stores $\sigma, \sigma' \in \Sigma$. Assume a
  variable $x$ and a value $v$ from any value domain. Now suppose
  \begin{itemize}
  \item $\sigma(x) = v$
  \item $\sigma'(x) = v$
  \item $(\sigma \setminus x) = (\sigma' \setminus x)$
  \end{itemize}
  Then $\sigma = sigma'$
\end{lem}
\begin{proof}
  By \coq{}.
\end{proof}

Another lemma on stores establishes a property when we are hiding
information:
\begin{lem}
\label{lem:hide_ne}
  Assume a store $\sigma \in \Sigma$ and two variables $x$ and
  $x'$. If $x \neq x'$ then we have $(\sigma \setminus x)(x') = \sigma(x')$.
\end{lem}
\begin{proof}
  In \coq{}, by the definition of hiding and case analysis.
\end{proof}

The following lemma will come in handy later. It is based on hiding
stores:
\begin{lem}
\label{lem:write_hide}
  Assume two stores $\sigma, \sigma' \in \Sigma$, a variable $x$ and
  two values $v_1, v_2 \in V$, for any value domain $V$. Then
  $\sigma[x \mapsto v_1] = \sigma'[x \mapsto v_2]$ implies $(\sigma
  \setminus x) = (\sigma' \setminus x)$
\end{lem}
\begin{proof}
  In \coq{}. The proof relies on backwards reasoning. It uses
  extensionality (see \ref{coqext:extensionality}) and then it uses
  case analysis. In the cases, it applies lemma \eqref{lem:hide_ne}.
\end{proof}

\subsection{History}

To be written. In this section we will describe some of the failed
attempts at formalizing the store we tried in the process. These are
interesting because they hint at the need for coming up with the
simple solutions rather than the complex or naive ones.

\fixme{write this!}

\section{Expressions in \janusz{}}

The \janusz{} language has a very simplified expression language
compared to full JANUS. In this language there are 5 expression
constructs: integer constants, store referencing, addition,
subtraction and multiplication.

The syntax of expressions $e$ is the following in BNF notation:
\newcommand{\bor}{\; | \;}
\begin{equation*}
  e ::= n \bor \mathtt{x} \bor e + e \bor e - e \bor e * e
\end{equation*}
The judgement forms are $\sigma |- e => z$ stating that under the
assumption of a store $\sigma$ the expression $e$ evaluates to the
integer $z$. The inference rules for this system is straightforward:
\begin{gather*}
  \inference[Const]{}{\sigma |- n => n} \quad \inference[Var]{\sigma(\mathtt{x}) =
    \lift{k}}{\sigma |- \mathtt{x} => k} \\
  \inference[Add]{\sigma |- e_1 => n_1 \quad \sigma |- e_2 => n_2 \quad
    n_1 + n_2 = n}{\sigma |- e_1 + e_2 => n} \\
  \inference[Sub]{\sigma |- e_1 => n_1 \quad \sigma |- e_2 => n_2 \quad
    n_1 - n_2 = n}{\sigma |- e_1 - e_2 => n} \\
  \inference[Mul]{\sigma |- e_1 => n_1 \quad \sigma |- e_2 => n_2 \quad
    n_1 * n_2 = n}{\sigma |- e_1 * e_2 => n}
\end{gather*}

There is another, denotational, semantics for expressions
however. This semantics are equal to the operational semantics given
above. One aspect of Coq is that it is more natural to express
denotational semantics than operational semantics.

For a denotational semantics, we define a computation
$\mathcal{E}|[-|] \colon E \to \Sigma \to \ZZ_{\perp}$ from an
expression and a Store $\Sigma$ to a lifted value. The definition is a
case analysis on the structure of the expression given:
\begin{align*}
  \mathcal{E}|[n|](\sigma) & = \lift{n}\\
  \mathcal{E}|[x|](\sigma) & = \sigma (x)\\
  \mathcal{E}|[e_1 + e_2|](\sigma) & = \mathcal{E}|[e_1|](\sigma) \;
  +_{\ZZ} \;
  \mathcal{E}|[e_2|](\sigma)\\
  \mathcal{E}|[e_1 - e_2|](\sigma) & = \mathcal{E}|[e_1|](\sigma) \;
  -_{\ZZ} \;
  \mathcal{E}|[e_2|](\sigma)\\
  \mathcal{E}|[e_1 * e_2|](\sigma) & = \mathcal{E}|[e_1|](\sigma) \;
  *_{\ZZ} \;
  \mathcal{E}|[e_2|](\sigma)
\end{align*}
The operation $+$ is the operation from our expression language
whereas the operation $+_{\ZZ}$ is the mathematical integer addition
here made explicit by its annotation. Likewise is the case for $-$ and
$*$.

The advantage of the latter, denotational, definition is that is
allows for simpler proofs in Coq. Basically a standard Case analysis
will do over the structure. Operational semantics uses a Prolog-style
where a relation between the premises and conclusion is defined. The
advantage of this prolog style is of course its generality. The
denotational style above is a function definition based on case
analysis which is a special case of a relation.

If $R \subseteq X \times Y$ is a relation on $X$ and $Y$, then the
relation would be a function in the folowing case: if $(a, b) \in R$
and also $(a, c) \in R$ then we must have $b = c$. In other words, a
function is deterministic in its output based on its output.

We will only need the forward determinism for expressions and hence
there is no need for the generality of the relation. We therefore
employ a denotational definition as it greatly simplifies the needed
proof of forward determinism:

\begin{thm}
  \janusz{} expressions are forward deterministic: suppose we have a
  store $\sigma$ and an expression $e$. Now let
  $\mathcal{E}|[e|](\sigma) = v_1$ for a value $v_1 \in \ZZ_{\perp}$
  and also let $\mathcal{E}|[e|](\sigma) = v_2$ for a value $v_2 \in
  \ZZ_{\perp}$. Then we have $v_1 = v_2$.
\end{thm}
\begin{proof}
By the use of \coq{} and the \texttt{grind}-tactic from
\cite{chlipala+08:cpdt}:
\begin{verbatim}
    Theorem exp_determ : forall m e v1 v2,
      denote_Exp m e = v1 -> denote_Exp m e = v2 -> v1 = v2.
    Proof.
      grind.
    Qed.
\end{verbatim}
\end{proof}
\fixme{Note that a functional denotational definition if deterministic
  by default!}
\section{Statements in \janusz{}}

\fixme{Skip is not described yet}
The statement syntax of \janusz{} defines a small, purely linear
subset of the full JANUS language. There are 4 constructs given via
the following notation in BNF:
\reservestyle{\command}{\mathbf}
\command{if[\;if\;],then[\;then\;],else[\;else\;],fi[\;fi\;]}
\begin{gather*}
  s ::= \quad x +\!\!= e \bor x -\!\!= e \bor s; s
  \bor \<if> e \<then> s \<else> s \<fi> e
\end{gather*}
We will interleave the description of each of these syntactical
elements with the inference rules for them. The judgement form for
execution of statements is $\sigma |- s -> \sigma'$ which designates that
under the assumption of a store $\sigma$, execution of statement $s$
will yield the altered store $\sigma'$.

The first operation is the increment operation of an element in the
store. This operation, written as $x +\!\!= e$ will evaluate the
expression $e$ to a number $k$ and then add this amount to the
location in the store to which $x$ points:
\begin{equation*}
  \inference[Inc0]{\sigma |- e => k \quad \sigma(x) = \lift{k'} \quad k +
    k' = n}{\sigma |- x +\!\!= e -> \sigma[x \mapsto n]}
\end{equation*}
In JANUS it is a requirement that the variable $x$ must not occur in
the expression $e$. The same requiremnt is present in \janusz{} and
has to do with the invertibility of such statements. There is,
however, an alternative semantics not present in the current
literature which directly encodes the requirement in the inference
rule.

Recall we defined an ability to ``hide'' certain variables in our
store. We can utilize this by hiding $x$ in the expression evaluation:
\begin{equation*}
  \inference[Inc]{(\sigma \setminus x) |- e => k \quad \sigma(x) =
    \lift{k'} \quad k + k' = n}{\sigma |- x +\!\!= e -> \sigma[x \mapsto n]}
\end{equation*}
Now, because expression evaluation of the ``Var'' case requires a
lifted value it is now impossible to construct an inference tree where
the expression $e$ refers to the value $x$. We have effectively
encoded the informal requirement into a formal one. This method,
correctness by construction, simplifies proof formalization: had we
chosen a predicate judgement for an non-occurring variable $x$, then
we would have to provide a proof with this added structure.

For subtraction, the definition is similar:
\begin{equation*}
  \inference[Sub]{(\sigma \setminus x) |- e => k \quad \sigma(x) =
    \lift{k'} \quad k - k' = n}{\sigma |- x -\!\!= e -> \sigma[x \mapsto n]}
\end{equation*}

The next rule is for sequencing operations. In the statement $s_1;
s_2$ one first executes $s_1$ and then feeds the resulting store into
the execution of $s_2$:
\begin{equation*}
  \inference[Seq]{\sigma |- s_1 -> \sigma'' \quad \sigma'' |- s_2 -> \sigma'}
  {\sigma |- s_1; s_2 -> \sigma'}
\end{equation*}

Finally, there is the rule for the branching instruction. In \janusz{}
the value $0$ is ``false'' and any value different from $0$ is the
``true'' value. This yields two rules, one for each case. The first
rule is for the ``false'' case:
\begin{equation*}
  \inference[If-false]{\sigma |- e_1 => 0 \quad \sigma |- s_2 -> \sigma'
    \quad \sigma' |- e_2 => 0}{\<if> e_1 \<then> s_1 \<else> s_2 \<fi>
    e_2 -> \sigma'}
\end{equation*}
This rule states that if the $e_1$ \emph{test} evaluates to a false
value, then the ``else''-branch is executed. Finally the
\emph{assertion} $e_2$ must be false as well.

The true case is similar, the difference being extra (mathematical)
requirements on what the expressions evaluates to:
\begin{equation*}
  \inference[If-true]{\sigma |- e_1 => k \quad k \neq 0 \quad \sigma |- s_1 -> \sigma'
    \quad \sigma' |- e_2 => k' \quad k' \neq 0}{\<if> e_1 \<then> s_1 \<else> s_2 \<fi>
    e_2 -> \sigma'}
\end{equation*}

The need for assertions has to do with reversibility, as we shall see
later on.

\subsection{Proving determinism of \janusz{}}

For \janusz{}, we will prove a couple of theorems. The 2 main theorems
to be proven are those of forward and backward determinism. The
language must be deterministic in the forward direction, so there is
only one possible outcome of any computation. Also, it must be
deterministic in the backwards direction: if we have a computation
resulting in a resulting store, the original store must be equivalent.

It is clear this establishes a necessary condition for
reversibility. Had the language not been backwards deterministic, it
would be outright impossible to reconstruct the input store from the
output store.

I claim that the condition is not sufficient however. My claim will be
justified after the following determinism proofs.

\begin{lem}
\label{j0-fwd-det-prime}
  Let $\sigma, \sigma' \in \Sigma$ be stores. Let $s$ be any \janusz{}
  statement. Now, if $\sigma |- s -> \sigma'$ implies for all $\sigma''
  \in \Sigma$ we have $\sigma |- s -> \sigma''$, then $\sigma' =
  \sigma''$. Formally:
  \begin{equation*}
    \forall \sigma, \sigma' \in \Sigma, s \in Stm \colon \sigma |- s
    -> \sigma' \implies (\forall \sigma'' \in \Sigma \colon \sigma |-
    s -> \sigma'' \implies \sigma' = \sigma'')
  \end{equation*}
\end{lem}
\begin{proof}
  In \coq{}.
\end{proof}

The proof proceeds by induction over the 1st dependent
hypothesis, ie $\sigma |- s -> \sigma'$. The increment and decrement
cases are similar. They proceed by noting (for increment) that
$(\sigma \setminus x) |- e => k$ occurs in the premise of $\sigma |-
s -> \sigma''$ as well, so the $k$ must be identical. The same is
the case for $\sigma(x) = \lift{k'}$ and hence $k + k'$ is written
in both cases.

The semi-case just aspires to the induction hypothesis. Due to the
way the lemma is worded we have $\forall \sigma'' \colon \sigma |- s
-> \sigma''$ as the induction hypothesis and then this is trivially
proven.

The if-case is done by inversion of $\sigma |- \<if> e1 \<then> s1
\<else> s2 \<fi> e2 -> \sigma''$. One of these are solvable by the
induction hypothesis. The other is solvable because it is an
impossible case (false-inversion occuring while proving the if-true
subgoal -- or true-inversion when proving the if-false subgoal).

\begin{thm}
\label{thm:j0-fwd-det}
  \janusz{} is forward deterministic, ie:
  \begin{equation*}
    \forall \sigma, \sigma', \sigma'' \in \Sigma, s \in Stm \colon \sigma |- s
    -> \sigma' \implies \sigma |- s -> \sigma'' \implies \sigma' = \sigma''
  \end{equation*}
\end{thm}
\begin{proof}
  The $\forall \sigma''...$ is hoistable in lemma
  \eqref{j0-fwd-det-prime} due to logic rules. This is made formal in
  \coq{} by introducing everything but $\sigma' = \sigma''$ as as
  hypotheses and then generalizing over the form needed to apply lemma
  \eqref{j0-fwd-det-prime}.
\end{proof}

In the forward determinism-proof nothing special has been applied to
carry out the proof. Either one can calculate (in the integer ring
$\ZZ$) and conclude the result by identification or one can apply the
induction hypothesis to obtain the result after inversion. As we shall
see, the backwards determistic proof is not as simple, though it
holds:

\begin{lem}
  Let $\sigma, \sigma' \in \Sigma$ be stores. Let $s \in Stm$ be any
  statement. Assume $\sigma' |- s -> \sigma$ (note the position of
  $\sigma'$, compare with lemma \eqref{j0-fwd-det-prime}). Then
  $\forall \sigma'' \colon \sigma'' |- s -> \sigma$ implies $\sigma' =
  \sigma''$. Formally:
  \begin{equation*}
    \forall \sigma, \sigma' \colon \sigma' |- s -> \sigma \implies
    (\forall \sigma'' \colon \sigma'' |- s -> \sigma \implies \sigma'
    = \sigma'')
  \end{equation*}
\end{lem}
\begin{proof}
  Via \coq{}
\end{proof}

Completing this proof requires considerably more ingenuity than the
forward proof. Again, we run induction on the 1st dependent
hypothesis. For the increment case, we first run inversion, and
identify equal terms via the \texttt{subst} tactic. We must have
$(\sigma' \setminus x) = (\sigma'' \setminus x)$ because we have
$\sigma'[x \mapsto k + k'] = \sigma''[x \mapsto k_0 + k_0']$ (lemma
\eqref{lem:write_hide}). This means, that $k = k_0$. Next, we have $k
+ k' = k_0 + k_0'$. This is because of lemma \eqref{lem:write-eq2} and
analysis on $\sigma'[x \mapsto k + k'] = \sigma''[x \mapsto k_0 +
k_0']$. Computation in the ring $\ZZ$ now gives that $k' =
k_0'$. After substitution, we then use lemma \eqref{lem:hide-eq}
proving the case. The decrement case is equivalent.

The cases using the induction hypothesis (semi/if) are trivial. But
note that they in no place utilize the fact that $e_2$ is being
evaluated. This supports my claim that backwards determinism is not
sufficient. \fixme{Check this!}

\begin{thm}
  \janusz{} is backward deterministic, ie:
  \begin{equation*}
    \forall \sigma, \sigma', \sigma'' \in \Sigma, s \in Stm \colon \sigma' |- s
    -> \sigma \implies \sigma'' |- s -> \sigma \implies \sigma' = \sigma''
  \end{equation*}
\end{thm}
\begin{proof}
  Like the proof of theorem \eqref{thm:j0-fwd-det}, we can hoist the
  forall-quantifier and formalize it in \coq{}.
\end{proof}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "master"
%%% End: 
